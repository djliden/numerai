{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerai Starter Kit",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPTIttwEUbgSwIeTV10+Fos",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djliden/numerai/blob/main/notebooks/regressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGlc6WXctP3Y"
      },
      "source": [
        "# 1 Introduction\n",
        "This notebook will walk you through the entire process of making a [numerai](numer.ai) submission, from downloading the data to submitting final predictions, all in a google colab notebook. In particular, it will address two challenges:\n",
        "- handling API keys in a remote environment (colab)\n",
        "- parsing the large CSV files which, if read all at once, will exceed colab's memory and cause the notebook to crash.\n",
        "\n",
        "This notebook will implement two models: a basic tabular neural network using `fastai` and a linear regression model using `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma62H4nY5DOt"
      },
      "source": [
        "## 1.1 Installing and Importing Dependencies\n",
        "First, we install and import the necessary packages. This cell is currently set *not* to print any output; if you run into any issues and need to check for error messages, comment out the `%%capture` line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fitBsr_ty8_"
      },
      "source": [
        "%%capture\n",
        "# install\n",
        "!pip install --upgrade python-dotenv numerapi\n",
        "\n",
        "# import dependencies\n",
        "import gc\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from getpass import getpass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "import sklearn.linear_model\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8k1mucsueRZ"
      },
      "source": [
        "## 1.2 Setting Up numerapi\n",
        "We will use the [numerapi](https://github.com/uuazed/numerapi) package to access the data and make submissions. For this to work, numerapi needs to use your API keys (which can be obtained [here](https://numer.ai/submit)). We will set up two main ways of passing these API keys to a numerapi instance:\n",
        "1. Read a `.env` file using the `python-dotenv` package. This will require you to upload a `.env` file (which contains your secret key and should *not* be kept under version control). Using this method means you will not have to directly enter your keys each time you use this notebook, though you will need to re-upload the `.env` file.\n",
        "2. Manually entering the API keys -- if you don't have access to, or don't want to mess with, your `.env` file.\n",
        "\n",
        "If you have a `.env` file, upload it to the default working directory, `content`, now. In either case, run the cell below to set up the numerapi instance. See [Appendix A](#app_a) for instructions on generating and downloading a .env file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z1CS2Uwv79C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2027c92e-7703-4f31-9806-e3272ce05a05"
      },
      "source": [
        "# Load the numerapi credentials from .env or prompt for them if not available\n",
        "def credential():\n",
        "    dotenv_path = find_dotenv()\n",
        "    load_dotenv(dotenv_path)\n",
        "\n",
        "    if os.getenv(\"NUMERAI_PUBLIC_KEY\"):\n",
        "        print(\"Loaded Numerai Public Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_PUBLIC_KEY\"] = getpass(\"Please enter your Numerai Public Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_SECRET_KEY\"):\n",
        "        print(\"Loaded Numerai Secret Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_SECRET_KEY\"] = getpass(\"Please enter your Numerai Secret Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_MODEL_ID\"):\n",
        "        print(\"Loaded Numerai Model ID into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_MODEL_ID\"] = getpass(\"Please enter your Numerai Model ID. You can find your key here: https://numer.ai/submit -> \")\n",
        "\n",
        "credential()\n",
        "public_key = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n",
        "secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n",
        "model_id = os.environ.get(\"NUMERAI_MODEL_ID\")\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\", public_id=public_key, secret_key=secret_key)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Numerai Public Key into Global Environment!\n",
            "Loaded Numerai Secret Key into Global Environment!\n",
            "Loaded Numerai Model ID into Global Environment!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1LhPvUS7RLk"
      },
      "source": [
        "You can read up on the functionality of numerapi [here](https://github.com/uuazed/numerapi). You can use it to download the competition data, view other numerai users' public profiles, check submission status, manage your stake, and much more. In this case, we'll only be using it to download competition data and submit predictions.\n",
        "\n",
        "## 1.3 Downloading Competition Data\n",
        "In a more structured project, you'll probably want to keep the data in a seprate directory from your scripts etc. You could also link google colab to your google drive and store the data there in order to avoid needing to download and process the data every time. In this case, however, we'll keep everything in `./content`, and download the data fresh each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcCNcVMv7y1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a75309ed-00ab-407e-9143-4f5aa697a4cd"
      },
      "source": [
        "napi.download_current_dataset()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-06 17:53:22,907 INFO numerapi.base_api: target file already exists\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./numerai_dataset_253.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm7lo6fe8Qsy"
      },
      "source": [
        "## 1.4 Generating the Training Sample\n",
        "\n",
        "If you look at the files we downloaded above, you'll see a `numerai_tournament_data.csv` file and a `numerai_training_data.csv` file. The \"tournament\" file contains many rows with targets which we can use for validation, so let's extract those and combine them with our training set. Note that this cell saves a new `csv` after combining the training and validation data, so we can avoid the time-consuming parsing process if we run this cell again in the same session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd06RGi4840k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0fd1d13-0303-4a32-91b9-bff2b72bfcb4"
      },
      "source": [
        "tourn_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_tournament_data.csv')\n",
        "train_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_training_data.csv')\n",
        "processed_train_file = Path('./training_processed.csv')\n",
        "\n",
        "if processed_train_file.exists():\n",
        "    print(\"Loading the processed training data from file\\n\")\n",
        "    training_data = pd.read_csv(processed_train_file)\n",
        "else:\n",
        "    tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "    val_df = pd.concat([chunk[chunk['data_type'] == 'validation'] for chunk in tqdm(tourn_iter_csv)])\n",
        "    tourn_iter_csv.close()\n",
        "    training_data = pd.read_csv(train_file)\n",
        "    training_data = pd.concat([training_data, val_df])\n",
        "    training_data.reset_index(drop=True, inplace=True)\n",
        "    print(\"Training Dataset Generated! Saving to file ...\")\n",
        "    training_data.to_csv(processed_train_file, index=False)\n",
        "\n",
        "\n",
        "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
        "target_cols = ['target']\n",
        "\n",
        "train_idx = training_data.index[training_data.data_type=='train'].tolist()\n",
        "val_idx = training_data.index[training_data.data_type=='validation'].tolist()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:36, 36.24s/it]\u001b[A\n",
            "2it [00:58, 29.15s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Generated! Saving to file ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BSr1BMR5Ilh"
      },
      "source": [
        "# 2 Modeling the Data\n",
        "\n",
        "In this section, we will define our evaluation metrics; run two different models (a linear regression model from `scikit-learn` and a neural network from `fastai`); and generate submission dataframes from those files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeYfQcry6svF"
      },
      "source": [
        "## 2.1 Evaluation Metrics\n",
        "\n",
        "In this section, we will define two key evaluation metrics used to assess the performance of models before submitting to the tournament. These metrics are:\n",
        "- Average Spearman Correlation per era: The sum of each era's Spearman correlation divided by the number of eras.\n",
        "- Sharpe Ratio: The average correlation per era divided by the standard deviation of the correlations per era.\n",
        "\n",
        "Both are defined in reasonable detail [here](https://wandb.ai/carlolepelaars/numerai_tutorial/reports/How-to-get-Started-With-Numerai--VmlldzoxODU0NTQ). The methods defined below are modified versions of the methods described in that post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFniCX98z6i"
      },
      "source": [
        "def corr(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the correlation by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The average per-era correlations.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() \n",
        "\n",
        "def sharpe(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe ratio by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The Sharpe ratio for your predictions.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() / corrs.std()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4azmVvR6ZhT"
      },
      "source": [
        "## 2.2 Cross Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnOaIHaW0rxi"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "train_sample = training_data.iloc[train_idx]\n",
        "X, y, era = train_sample[feature_cols], train_sample.target, train_sample.era\n",
        "groups = train_sample.era\n",
        "group_kfold = GroupKFold(n_splits = 5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3qwFNYGF5TB"
      },
      "source": [
        "## 2.3 Linear Regression Model\n",
        "This model closely follows the tutorial example [here](https://colab.research.google.com/github/numerai/example-scripts/blob/master/making-your-first-submission-on-numerai.ipynb). We will use the `scikit-learn` package, with which we can implement and fit our regression model in just a couple of lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-el1HtBefnb"
      },
      "source": [
        "#### Fitting the Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKG75I7-yKvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca388973-13fb-4fd2-b1ed-ba359d838bdf"
      },
      "source": [
        "%%time\n",
        "corrs = []\n",
        "sharpes = []\n",
        "\n",
        "for train, test in group_kfold.split(X, y, groups=groups):\n",
        "    model = sklearn.linear_model.LinearRegression(n_jobs = -1)\n",
        "    model.fit(X.iloc[train], y.iloc[train])\n",
        "    val_preds = model.predict(X.iloc[test])\n",
        "    eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':y.iloc[test],\n",
        "                        'era':groups.iloc[test]}).reset_index()\n",
        "    corrs.append(corr(eval_df))\n",
        "    sharpes.append(sharpe(eval_df))\n",
        "\n",
        "print(corrs)\n",
        "print(sharpes)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0413988755478284, 0.03116761404394065, 0.03960886988203633, 0.03559675523170117, 0.031243561514435816]\n",
            "[1.5032811853317405, 0.7154594471580004, 1.8264734992258658, 1.0864696284517816, 1.0273427745585324]\n",
            "CPU times: user 1min 18s, sys: 6.92 s, total: 1min 25s\n",
            "Wall time: 48.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fL4MLgxKYHB"
      },
      "source": [
        "models = [\n",
        "          # sklearn.linear_model.LinearRegression(n_jobs = -1),\n",
        "          # sklearn.linear_model.Lasso(alpha=0.00006), # good! Takes pretty long time.\n",
        "          # sklearn.linear_model.Lasso(alpha=0.00001), # Takes a long time, worse than .00005\n",
        "          # sklearn.linear_model.Lasso(alpha=0.000005), # Takes a really long time, worse than .00001\n",
        "          # sklearn.linear_model.Lasso(alpha=0.01), # fails\n",
        "          # sklearn.linear_model.Ridge(alpha=0.0001),\n",
        "          # sklearn.linear_model.Ridge(alpha=0.01),\n",
        "          # sklearn.linear_model.Ridge(alpha = 0.1),\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.5),  # Good!\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0002, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.4),\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.6), # Good! Best?\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.7) # Good! Best?\n",
        "\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.00005, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0005, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.00001, l1_ratio=0.25) # Takes very long time\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.01, l1_ratio=0.5), # fails\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.001, l1_ratio=0.25),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.001, l1_ratio=0.75) # bad result\n",
        "          ]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RuK4TFtKX3l",
        "outputId": "2ede813b-8dde-415c-fe03-6c99ac07710e"
      },
      "source": [
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'\\nmodel: {model.__class__.__name__}')\n",
        "    if model.__class__.__name__!=\"LinearRegression\":\n",
        "        print(f'alpha: {model.alpha}')\n",
        "    if model.__class__.__name__==\"ElasticNet\":\n",
        "        print(f'L1 Ratio: {model.l1_ratio}')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 1/5 [01:09<04:36, 69.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [02:00<03:11, 63.85s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [03:08<02:10, 65.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [04:07<01:03, 63.18s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [05:17<00:00, 63.49s/it]\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: Lasso\n",
            "alpha: 6e-05\n",
            "validation correlations: [0.04332792912546912, 0.031434612619569154, 0.047669425200041284, 0.037039767734678784, 0.033323948348046604], mean: 0.03855913660556099\n",
            "validation sharpes: [1.3982035171750715, 0.663410919086826, 2.0610600121461347, 0.9854986745433042, 0.9674671028213477], mean: 1.215128045154537\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:26<01:47, 26.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [00:50<01:17, 25.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [02:00<01:18, 39.11s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [02:24<00:34, 34.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [03:28<00:00, 41.75s/it]\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.5\n",
            "validation correlations: [0.04347474676845641, 0.03146511092526201, 0.047107530862073116, 0.03730122819900296, 0.03341224276698343], mean: 0.03855217190435559\n",
            "validation sharpes: [1.416323394305138, 0.6677888412291337, 2.0469916387510954, 1.0044445499519083, 0.9793182577424634], mean: 1.2229733363959476\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:42<02:49, 42.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [01:12<01:55, 38.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [01:56<01:20, 40.41s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [03:07<00:49, 49.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [04:27<00:00, 53.56s/it]\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.6\n",
            "validation correlations: [0.04333048511430412, 0.0314392864379456, 0.04768023730348006, 0.037036624347257956, 0.033322480554536356], mean: 0.03856182275150481\n",
            "validation sharpes: [1.398431288545246, 0.6634818817752716, 2.061180211216678, 0.985120477558037, 0.9673392231760424], mean: 1.2151106164542549\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:43<02:54, 43.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [01:38<02:20, 46.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [02:50<01:49, 54.66s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [03:21<00:47, 47.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [04:35<00:00, 55.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.7\n",
            "validation correlations: [0.04306784559555605, 0.031391661906845475, 0.047987359704428266, 0.03667405756155036, 0.033069786367601856], mean: 0.038438142227196395\n",
            "validation sharpes: [1.375155679465384, 0.6616136201502878, 2.0557192796959773, 0.965906404669539, 0.9535450170076238], mean: 1.2023880001977623\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLxlDiV_Q6ao"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "import sklearn.neighbors\n",
        "models = [        \n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, n_jobs = -1),\n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, leaf_size = 150, n_jobs = -1),\n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, leaf_size = 10, n_jobs = -1)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__}')\n",
        "    print(f'neighbors: {model.n_neighbors}')\n",
        "    print(f'leaf size: {model.leaf_size}')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvd-j2Xj3EZ",
        "outputId": "fee07261-25e1-4b5d-a2ee-dbcc25f782d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 976
        }
      },
      "source": [
        "import sklearn.ensemble\n",
        "models = [\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=5, max_features = 0.2, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=1000, max_features = 0.2, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=100, max_features = 0.8, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=1000, max_features = 0.8, n_jobs=-1)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__} with {model.n_estimators} estimators and {model.max_features} max features.')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 20%|██        | 1/5 [01:21<05:26, 81.64s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 40%|████      | 2/5 [02:45<04:06, 82.22s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 60%|██████    | 3/5 [04:05<02:43, 81.60s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            " 80%|████████  | 4/5 [05:27<01:21, 81.61s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n",
            "\n",
            "\n",
            "\n",
            "100%|██████████| 5/5 [06:49<00:00, 81.94s/it]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ExtraTreesRegressor with 5 estimators and 0.2 max features.\n",
            "validation correlations: [0.0010127576097305518, -0.0019432661133448404, -0.001050717229811398, -0.006903266680195498, -0.005073140109470949], mean: -0.002791526504618427\n",
            "validation sharpes: [0.043507529034795836, -0.0851838765404326, -0.06257506604678303, -0.3824174354022222, -0.20594826498859695], mean: -0.1385234227886478\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-925f578a4d0a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msharpes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         eval_df = pd.DataFrame({'prediction':val_preds,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 383\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    384\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1052\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    931\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 648\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    294\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inUbdhvb66Pb"
      },
      "source": [
        "#### Assessing Regression Model Performance\n",
        "\n",
        "Here we apply the `corr` and `sharpe` methods defined above to predictions made on the validation sample in order to estimate our model's tournament performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgU0Pz9x9vpv"
      },
      "source": [
        "val_sample = training_data.iloc[val_idx]\n",
        "val_preds = model.predict(val_sample[feature_cols])\n",
        "eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':val_sample.target,\n",
        "                        'era':val_sample.era}).reset_index()\n",
        "val_sharpe = sharpe(eval_df)\n",
        "val_corr = corr(eval_df)\n",
        "\n",
        "print((f'The linear regression model\\'s validation correlation is {val_corr}. '\n",
        "       f'Its validation sharpe is {val_sharpe}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLGopZF0r7T"
      },
      "source": [
        "#### Making Predictions with the Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOeJjJdZ08aV"
      },
      "source": [
        "ids = []\n",
        "preds = []\n",
        "\n",
        "chunksize = 50000\n",
        "\n",
        "tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "for chunk in tourn_iter_csv:\n",
        "    df = chunk[feature_cols]\n",
        "    out = model.predict(df)\n",
        "    ids.extend(chunk[\"id\"])\n",
        "    preds.extend(out)\n",
        "tourn_iter_csv.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HN8lrw2zd_"
      },
      "source": [
        "linear_regression_predictions_df = pd.DataFrame({\n",
        "    'id':ids,\n",
        "    'prediction':preds\n",
        "})\n",
        "linear_regression_predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg_HdPki6Mh3"
      },
      "source": [
        "linear_regression_predictions_df.to_csv(\"lr_predictions.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEwbRegtWrRE"
      },
      "source": [
        "#### Submitting Predictions from the Linear Regression Model\n",
        "\n",
        "We can use `numerapi` to submit these predictions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW-zccdgW0GX"
      },
      "source": [
        "napi.upload_predictions(\"lr_predictions.csv\", model_id=os.environ.get(\"NUMERAI_MODEL_ID\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}