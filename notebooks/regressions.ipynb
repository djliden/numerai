{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerai Starter Kit",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMTtInQOPZN3SFPyFQgy/PR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djliden/numerai/blob/main/notebooks/regressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGlc6WXctP3Y"
      },
      "source": [
        "# 1 Introduction\n",
        "This notebook will walk you through the entire process of making a [numerai](numer.ai) submission, from downloading the data to submitting final predictions, all in a google colab notebook. In particular, it will address two challenges:\n",
        "- handling API keys in a remote environment (colab)\n",
        "- parsing the large CSV files which, if read all at once, will exceed colab's memory and cause the notebook to crash.\n",
        "\n",
        "This notebook will implement two models: a basic tabular neural network using `fastai` and a linear regression model using `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma62H4nY5DOt"
      },
      "source": [
        "## 1.1 Installing and Importing Dependencies\n",
        "First, we install and import the necessary packages. This cell is currently set *not* to print any output; if you run into any issues and need to check for error messages, comment out the `%%capture` line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fitBsr_ty8_"
      },
      "source": [
        "%%capture\n",
        "# install\n",
        "!pip install --upgrade python-dotenv numerapi\n",
        "\n",
        "# import dependencies\n",
        "import gc\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from getpass import getpass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "import sklearn.linear_model\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8k1mucsueRZ"
      },
      "source": [
        "## 1.2 Setting Up numerapi\n",
        "We will use the [numerapi](https://github.com/uuazed/numerapi) package to access the data and make submissions. For this to work, numerapi needs to use your API keys (which can be obtained [here](https://numer.ai/submit)). We will set up two main ways of passing these API keys to a numerapi instance:\n",
        "1. Read a `.env` file using the `python-dotenv` package. This will require you to upload a `.env` file (which contains your secret key and should *not* be kept under version control). Using this method means you will not have to directly enter your keys each time you use this notebook, though you will need to re-upload the `.env` file.\n",
        "2. Manually entering the API keys -- if you don't have access to, or don't want to mess with, your `.env` file.\n",
        "\n",
        "If you have a `.env` file, upload it to the default working directory, `content`, now. In either case, run the cell below to set up the numerapi instance. See [Appendix A](#app_a) for instructions on generating and downloading a .env file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z1CS2Uwv79C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc5ee956-e4fb-4f35-83b7-0a384f6f3821"
      },
      "source": [
        "# Load the numerapi credentials from .env or prompt for them if not available\n",
        "def credential():\n",
        "    dotenv_path = find_dotenv()\n",
        "    load_dotenv(dotenv_path)\n",
        "\n",
        "    if os.getenv(\"NUMERAI_PUBLIC_KEY\"):\n",
        "        print(\"Loaded Numerai Public Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_PUBLIC_KEY\"] = getpass(\"Please enter your Numerai Public Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_SECRET_KEY\"):\n",
        "        print(\"Loaded Numerai Secret Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_SECRET_KEY\"] = getpass(\"Please enter your Numerai Secret Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_MODEL_ID_REGRESSIONS\"):\n",
        "        print(\"Loaded Numerai Model ID into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_MODEL_ID_REGRESSIONS\"] = getpass(\"Please enter your Numerai Model ID. You can find your key here: https://numer.ai/submit -> \")\n",
        "\n",
        "credential()\n",
        "public_key = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n",
        "secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n",
        "model_id = os.environ.get(\"NUMERAI_MODEL_ID_REGRESSIONS\")\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\", public_id=public_key, secret_key=secret_key)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Numerai Public Key into Global Environment!\n",
            "Loaded Numerai Secret Key into Global Environment!\n",
            "Loaded Numerai Model ID into Global Environment!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1LhPvUS7RLk"
      },
      "source": [
        "You can read up on the functionality of numerapi [here](https://github.com/uuazed/numerapi). You can use it to download the competition data, view other numerai users' public profiles, check submission status, manage your stake, and much more. In this case, we'll only be using it to download competition data and submit predictions.\n",
        "\n",
        "## 1.3 Downloading Competition Data\n",
        "In a more structured project, you'll probably want to keep the data in a seprate directory from your scripts etc. You could also link google colab to your google drive and store the data there in order to avoid needing to download and process the data every time. In this case, however, we'll keep everything in `./content`, and download the data fresh each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcCNcVMv7y1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "71f21449-a137-4b59-83d2-75efd808cfbf"
      },
      "source": [
        "napi.download_current_dataset()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./numerai_dataset_254.zip: 100%|█████████▉| 394M/394M [00:11<00:00, 41.4MB/s]2021-03-13 13:40:19,010 INFO numerapi.base_api: unzipping file...\n",
            "./numerai_dataset_254.zip: 394MB [00:29, 41.4MB/s]                           "
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./numerai_dataset_254.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm7lo6fe8Qsy"
      },
      "source": [
        "## 1.4 Generating the Training Sample\n",
        "\n",
        "If you look at the files we downloaded above, you'll see a `numerai_tournament_data.csv` file and a `numerai_training_data.csv` file. The \"tournament\" file contains many rows with targets which we can use for validation, so let's extract those and combine them with our training set. Note that this cell saves a new `csv` after combining the training and validation data, so we can avoid the time-consuming parsing process if we run this cell again in the same session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd06RGi4840k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07bdff23-f9c7-41e9-aba5-d7c4a6724227"
      },
      "source": [
        "tourn_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_tournament_data.csv')\n",
        "train_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_training_data.csv')\n",
        "processed_train_file = Path('./training_processed.csv')\n",
        "\n",
        "if processed_train_file.exists():\n",
        "    print(\"Loading the processed training data from file\\n\")\n",
        "    training_data = pd.read_csv(processed_train_file)\n",
        "else:\n",
        "    tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "    val_df = pd.concat([chunk[chunk['data_type'] == 'validation'] for chunk in tqdm(tourn_iter_csv)])\n",
        "    tourn_iter_csv.close()\n",
        "    training_data = pd.read_csv(train_file)\n",
        "    training_data = pd.concat([training_data, val_df])\n",
        "    training_data.reset_index(drop=True, inplace=True)\n",
        "    print(\"Training Dataset Generated! Saving to file ...\")\n",
        "    training_data.to_csv(processed_train_file, index=False)\n",
        "\n",
        "\n",
        "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
        "target_cols = ['target']\n",
        "\n",
        "train_idx = training_data.index[training_data.data_type=='train'].tolist()\n",
        "val_idx = training_data.index[training_data.data_type=='validation'].tolist()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "0it [00:00, ?it/s]\u001b[A\n",
            "1it [00:37, 37.26s/it]\u001b[A\n",
            "2it [00:59, 30.00s/it]\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training Dataset Generated! Saving to file ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BSr1BMR5Ilh"
      },
      "source": [
        "# 2 Modeling the Data\n",
        "\n",
        "In this section, we will define our evaluation metrics; run two different models (a linear regression model from `scikit-learn` and a neural network from `fastai`); and generate submission dataframes from those files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeYfQcry6svF"
      },
      "source": [
        "## 2.1 Evaluation Metrics\n",
        "\n",
        "In this section, we will define two key evaluation metrics used to assess the performance of models before submitting to the tournament. These metrics are:\n",
        "- Average Spearman Correlation per era: The sum of each era's Spearman correlation divided by the number of eras.\n",
        "- Sharpe Ratio: The average correlation per era divided by the standard deviation of the correlations per era.\n",
        "\n",
        "Both are defined in reasonable detail [here](https://wandb.ai/carlolepelaars/numerai_tutorial/reports/How-to-get-Started-With-Numerai--VmlldzoxODU0NTQ). The methods defined below are modified versions of the methods described in that post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFniCX98z6i"
      },
      "source": [
        "def corr(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the correlation by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The average per-era correlations.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() \n",
        "\n",
        "def sharpe(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe ratio by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The Sharpe ratio for your predictions.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() / corrs.std()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4azmVvR6ZhT"
      },
      "source": [
        "## 2.2 Cross Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnOaIHaW0rxi"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "from sklearn.model_selection import GroupKFold\n",
        "train_sample = training_data.iloc[train_idx]\n",
        "X, y, era = train_sample[feature_cols], train_sample.target, train_sample.era\n",
        "groups = train_sample.era\n",
        "group_kfold = GroupKFold(n_splits = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlU4iHvIzczO"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "# ALTERNATE SETUP\n",
        "from sklearn.model_selection import GroupKFold\n",
        "train_sample = training_data\n",
        "X, y, era = train_sample[feature_cols], train_sample.target, train_sample.era\n",
        "groups = train_sample.era\n",
        "group_kfold = GroupKFold(n_splits = 5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-9-4GmFvjMY"
      },
      "source": [
        "### 2.2.1 Custom Cross Validation Setup\n",
        "\n",
        "The goal of this section is to set up a \"group time series\" approach where we specify a certain set of \"eras\" for training with the last era for validation. We will be training on segments of the validation set.\n",
        "\n",
        "There are a few ways to do this; I want to write a class that can take the \"eras to test on\" as input and return CV folds as outout. Perhaps a future refinement would include an argument for number of eras validate on. Perhaps unnecessary given that the \"real\" task is testing on a single era. Or four eras?\n",
        "\n",
        "#### Usage\n",
        "\n",
        "1. Initialize the class with the eras column: `cv = EraCV(training_data.era)`\n",
        "2. Get splits: `X, y = test.get_splits(valid_start = 80, valid_n_eras = 4, train_n_eras = None)`\n",
        "\n",
        "The `valid_start` argument identifies the first training era; it takes an integer value. `valid_n_eras` is the number of eras to include in the validation set. `train_n_eras` is the number of eras to include in the training set. `train_n_eras` before `valid_start` are included in the training set. If no argument is passed to `train_n_eras`, all eras from 0 to `valid_start` are included.\n",
        "\n",
        "A single instance of this class can be used in a loop to generate multiple train/test splits. Assuming you want to keep the number of train and test eras constant, you can just iterate over a list of validation starting eras.\n",
        "\n",
        "Features such as checking if a given validation era actually exists have not yet been implemented."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hy_qJIWyA4i"
      },
      "source": [
        "class EraCV:\n",
        "    \"\"\"Select validation eras and train on previous eras\n",
        "\n",
        "    provides train/test indices to split data in train/test splits. In\n",
        "    each split, one or more eras are used as a validation set while the\n",
        "    specified number of immediately preceding eras are used as a\n",
        "    training set.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, eras):\n",
        "        self.eras = eras\n",
        "        self.unique_eras = _era_to_int(eras.unique())\n",
        "        self.eras_int = _era_to_int(eras)\n",
        "        #self.valid_start = valid_start\n",
        "        #self.valid_n_eras = valid_n_eras\n",
        "        #self.train_n_eras = 0 if (train_n_eras is None) else train_n_eras\n",
        "    \n",
        "    def _era_to_int(eras):\n",
        "        return np.array([int(era[3:]) for era in eras])\n",
        "\n",
        "    def get_valid_indices(self, valid_start, valid_n_eras):\n",
        "        self.valid_eras = self.unique_eras[self.unique_eras.index(valid_start):\\\n",
        "                                      self.unique_eras.index(valid_start)+\\\n",
        "                                      valid_n_eras]\n",
        "        valid_bool = [era in self.valid_eras for era in self.eras_int] \n",
        "        self.valid_indices = np.where(valid_bool)\n",
        "\n",
        "    def get_train_indices(self, valid_start:int, train_n_eras:int):\n",
        "        train_n_eras = 0 if (train_n_eras is None) else train_n_eras\n",
        "        self.train_eras = [era for era in self.unique_eras if era <\\\n",
        "                           valid_start][-train_n_eras:]\n",
        "        train_bool = [era in self.train_eras for era in self.eras_int]\n",
        "        self.train_indices = np.where(train_bool)\n",
        "\n",
        "    def get_splits(self, valid_start:int, valid_n_eras:int,\n",
        "                   train_n_eras:int = None):\n",
        "        self.get_valid_indices(valid_start, valid_n_eras)\n",
        "        self.get_train_indices(valid_start, train_n_eras)\n",
        "        return self.train_indices[0], self.valid_indices[0]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3qwFNYGF5TB"
      },
      "source": [
        "## 2.3 Linear Regression Model\n",
        "This model closely follows the tutorial example [here](https://colab.research.google.com/github/numerai/example-scripts/blob/master/making-your-first-submission-on-numerai.ipynb). We will use the `scikit-learn` package, with which we can implement and fit our regression model in just a couple of lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-el1HtBefnb"
      },
      "source": [
        "#### Fitting the Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKG75I7-yKvA"
      },
      "source": [
        "%%time\n",
        "corrs = []\n",
        "sharpes = []\n",
        "era_split = EraCV(eras = training_data.era)\n",
        "X, y, era = training_data[feature_cols], training_data.target, training_data.era\n",
        "for valid_era in [119, 130, 197]:\n",
        "    train, test = era_split.get_splits(valid_start = valid_era,\n",
        "                           valid_n_eras = 4,\n",
        "                           train_n_eras = None)\n",
        "    model = sklearn.linear_model.LinearRegression(n_jobs = -1)\n",
        "    model.fit(X.iloc[train], y.iloc[train])\n",
        "    val_preds = model.predict(X.iloc[test])\n",
        "    eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':y.iloc[test],\n",
        "                        'era':era.iloc[test]}).reset_index()\n",
        "    corrs.append(corr(eval_df))\n",
        "    sharpes.append(sharpe(eval_df))\n",
        "\n",
        "print(corrs)\n",
        "print(sharpes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fL4MLgxKYHB"
      },
      "source": [
        "models = [\n",
        "          sklearn.linear_model.LinearRegression(n_jobs = -1),\n",
        "          # sklearn.linear_model.Lasso(alpha=0.00006), # good! Takes pretty long time.\n",
        "          # sklearn.linear_model.Lasso(alpha=0.00001), # Takes a long time, worse than .00005\n",
        "          # sklearn.linear_model.Lasso(alpha=0.000005), # Takes a really long time, worse than .00001\n",
        "          # sklearn.linear_model.Lasso(alpha=0.01), # fails\n",
        "          sklearn.linear_model.Ridge(alpha=0.0001),\n",
        "          # sklearn.linear_model.Ridge(alpha=0.01),\n",
        "          # sklearn.linear_model.Ridge(alpha = 0.1),\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.5),  # Good!\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0002, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.4),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.6), # Good! Best?\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.7) # Good! Best?\n",
        "\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.00005, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0005, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.00001, l1_ratio=0.25) # Takes very long time\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.01, l1_ratio=0.5), # fails\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.001, l1_ratio=0.25),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.001, l1_ratio=0.75) # bad result\n",
        "          ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RuK4TFtKX3l",
        "outputId": "a588a982-61d2-4c00-98d6-17ad0da3a63d"
      },
      "source": [
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'\\nmodel: {model.__class__.__name__}')\n",
        "    if model.__class__.__name__!=\"LinearRegression\":\n",
        "        print(f'alpha: {model.alpha}')\n",
        "    if model.__class__.__name__==\"ElasticNet\":\n",
        "        print(f'L1 Ratio: {model.l1_ratio}')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\n",
            " 20%|██        | 1/5 [00:17<01:10, 17.52s/it]\u001b[A\n",
            " 40%|████      | 2/5 [00:32<00:49, 16.66s/it]\u001b[A\n",
            " 60%|██████    | 3/5 [00:45<00:31, 15.69s/it]\u001b[A\n",
            " 80%|████████  | 4/5 [00:58<00:14, 14.95s/it]\u001b[A\n",
            "100%|██████████| 5/5 [01:12<00:00, 14.40s/it]\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model: LinearRegression\n",
            "validation correlations: [0.030592672832642553, 0.029802794658032174, 0.03535880547271921, 0.033360716209074485, 0.03270394557743098], mean: 0.032363786949979885\n",
            "validation sharpes: [1.114464375154921, 0.9485257250293379, 0.9992376387806917, 1.24032712933418, 0.7823271632464406], mean: 1.0169764063091145\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 1/5 [00:03<00:12,  3.16s/it]\u001b[A\n",
            " 40%|████      | 2/5 [00:05<00:08,  3.00s/it]\u001b[A\n",
            " 60%|██████    | 3/5 [00:08<00:05,  2.92s/it]\u001b[A\n",
            " 80%|████████  | 4/5 [00:11<00:02,  2.86s/it]\u001b[A\n",
            "100%|██████████| 5/5 [00:13<00:00,  2.79s/it]\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model: Ridge\n",
            "alpha: 0.0001\n",
            "validation correlations: [0.030592688376691327, 0.029802788108775426, 0.03535880547271921, 0.03336072298313674, 0.032703961724224004], mean: 0.03236379333310935\n",
            "validation sharpes: [1.1144649516401044, 0.9485252354835974, 0.9992376387806917, 1.2403277698874937, 0.7823276682925802], mean: 1.0169766528168933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " 20%|██        | 1/5 [00:18<01:14, 18.75s/it]\u001b[A\n",
            " 40%|████      | 2/5 [00:40<00:58, 19.59s/it]\u001b[A\n",
            " 60%|██████    | 3/5 [00:58<00:38, 19.14s/it]\u001b[A\n",
            " 80%|████████  | 4/5 [01:17<00:19, 19.07s/it]\u001b[A\n",
            "100%|██████████| 5/5 [01:37<00:00, 19.43s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.5\n",
            "validation correlations: [0.0321315350849346, 0.030686260652279727, 0.03937208763992656, 0.03557058382127742, 0.03677966987267029], mean: 0.03490802741421772\n",
            "validation sharpes: [1.0837542294691465, 0.8349751370328967, 0.9651805955650611, 1.1853246741830534, 0.7697754775344541], mean: 0.9678020227569224\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLxlDiV_Q6ao"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "# Takes a very long time\n",
        "import sklearn.neighbors\n",
        "models = [        \n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, n_jobs = -1),\n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, leaf_size = 150, n_jobs = -1),\n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, leaf_size = 10, n_jobs = -1)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__}')\n",
        "    print(f'neighbors: {model.n_neighbors}')\n",
        "    print(f'leaf size: {model.leaf_size}')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvd-j2Xj3EZ"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "# Takes a very long time\n",
        "import sklearn.ensemble\n",
        "models = [\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=5, max_features = 0.8, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=10, max_features = 0.8, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=5, max_features = 0.2, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=10, max_features = 0.2, n_jobs=-1),\n",
        "          # sklearn.ensemble.ExtraTreesRegressor(n_estimators=100, max_features = 0.8, n_jobs=-1),\n",
        "          # sklearn.ensemble.ExtraTreesRegressor(n_estimators=1000, max_features = 0.8, n_jobs=-1)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__} with {model.n_estimators} estimators and {model.max_features} max features.')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "Af4VMqC9r7tY",
        "outputId": "8a02caec-a558-4b28-adb1-bd6fe21ddf8f"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "\n",
        "import sklearn.ensemble\n",
        "models = [\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=50, max_features = 0.8, subsample=0.1),\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, max_features = 0.8),\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=50, max_features = 0.2),\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, max_features = 0.2)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__} with {model.n_estimators} estimators and {model.max_features} max features.')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-de2b0775e7fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msharpes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         eval_df = pd.DataFrame({'prediction':val_preds,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1245\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inUbdhvb66Pb"
      },
      "source": [
        "#### Assessing Regression Model Performance\n",
        "\n",
        "Here we apply the `corr` and `sharpe` methods defined above to predictions made on the validation sample in order to estimate our model's tournament performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgU0Pz9x9vpv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bbad879-d1a3-4e8b-89d6-df60d32614ee"
      },
      "source": [
        "# Fit final model\n",
        "X, y, era = train_sample[feature_cols], train_sample.target, train_sample.era\n",
        "model = sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.6)\n",
        "model.fit(X, y)\n",
        "val_sample = training_data.iloc[val_idx]\n",
        "val_preds = model.predict(val_sample[feature_cols])\n",
        "eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':val_sample.target,\n",
        "                        'era':val_sample.era}).reset_index()\n",
        "val_sharpe = sharpe(eval_df)\n",
        "val_corr = corr(eval_df)\n",
        "\n",
        "print((f'The linear regression model\\'s validation correlation is {val_corr}. '\n",
        "       f'Its validation sharpe is {val_sharpe}'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The linear regression model's validation correlation is 0.016638328482573635. Its validation sharpe is 0.4802300833842811\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLGopZF0r7T"
      },
      "source": [
        "#### Making Predictions with the Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOeJjJdZ08aV"
      },
      "source": [
        "ids = []\n",
        "preds = []\n",
        "\n",
        "chunksize = 50000\n",
        "\n",
        "tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "for chunk in tourn_iter_csv:\n",
        "    df = chunk[feature_cols]\n",
        "    out = model.predict(df)\n",
        "    ids.extend(chunk[\"id\"])\n",
        "    preds.extend(out)\n",
        "tourn_iter_csv.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HN8lrw2zd_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "568ea403-336a-4255-81ba-551fbd129f16"
      },
      "source": [
        "linear_regression_predictions_df = pd.DataFrame({\n",
        "    'id':ids,\n",
        "    'prediction':preds\n",
        "})\n",
        "linear_regression_predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>n0003aa52cab36c2</td>\n",
              "      <td>0.481178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>n000920ed083903f</td>\n",
              "      <td>0.492734</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>n0038e640522c4a6</td>\n",
              "      <td>0.524351</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>n004ac94a87dc54b</td>\n",
              "      <td>0.492209</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>n0052fe97ea0c05f</td>\n",
              "      <td>0.502477</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 id  prediction\n",
              "0  n0003aa52cab36c2    0.481178\n",
              "1  n000920ed083903f    0.492734\n",
              "2  n0038e640522c4a6    0.524351\n",
              "3  n004ac94a87dc54b    0.492209\n",
              "4  n0052fe97ea0c05f    0.502477"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg_HdPki6Mh3"
      },
      "source": [
        "linear_regression_predictions_df.to_csv(\"lr_predictions.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEwbRegtWrRE"
      },
      "source": [
        "#### Submitting Predictions from the Linear Regression Model\n",
        "\n",
        "We can use `numerapi` to submit these predictions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW-zccdgW0GX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "7881d626-2de1-4cee-834b-02b9a5a7f789"
      },
      "source": [
        "napi.upload_predictions(\"lr_predictions.csv\", model_id=os.environ.get(\"NUMERAI_MODEL_ID_REGRESSIONS\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-06 18:56:10,116 INFO numerapi.base_api: uploading predictions...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'6e5fcc02-19f6-43eb-8aaf-3061dea74c13'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJzBmQoQ5aD5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}