{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Numerai Starter Kit",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM+vIemF33hUpPv31ts7w4E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/djliden/numerai/blob/main/notebooks/regressions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGlc6WXctP3Y"
      },
      "source": [
        "# 1 Introduction\n",
        "This notebook will walk you through the entire process of making a [numerai](numer.ai) submission, from downloading the data to submitting final predictions, all in a google colab notebook. In particular, it will address two challenges:\n",
        "- handling API keys in a remote environment (colab)\n",
        "- parsing the large CSV files which, if read all at once, will exceed colab's memory and cause the notebook to crash.\n",
        "\n",
        "This notebook will implement two models: a basic tabular neural network using `fastai` and a linear regression model using `scikit-learn`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ma62H4nY5DOt"
      },
      "source": [
        "## 1.1 Installing and Importing Dependencies\n",
        "First, we install and import the necessary packages. This cell is currently set *not* to print any output; if you run into any issues and need to check for error messages, comment out the `%%capture` line"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6fitBsr_ty8_"
      },
      "source": [
        "%%capture\n",
        "# install\n",
        "!pip install --upgrade python-dotenv numerapi\n",
        "\n",
        "# import dependencies\n",
        "import gc\n",
        "import os\n",
        "from dotenv import load_dotenv, find_dotenv\n",
        "from getpass import getpass\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import numerapi\n",
        "from pathlib import Path\n",
        "from scipy.stats import spearmanr\n",
        "import sklearn.linear_model\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8k1mucsueRZ"
      },
      "source": [
        "## 1.2 Setting Up numerapi\n",
        "We will use the [numerapi](https://github.com/uuazed/numerapi) package to access the data and make submissions. For this to work, numerapi needs to use your API keys (which can be obtained [here](https://numer.ai/submit)). We will set up two main ways of passing these API keys to a numerapi instance:\n",
        "1. Read a `.env` file using the `python-dotenv` package. This will require you to upload a `.env` file (which contains your secret key and should *not* be kept under version control). Using this method means you will not have to directly enter your keys each time you use this notebook, though you will need to re-upload the `.env` file.\n",
        "2. Manually entering the API keys -- if you don't have access to, or don't want to mess with, your `.env` file.\n",
        "\n",
        "If you have a `.env` file, upload it to the default working directory, `content`, now. In either case, run the cell below to set up the numerapi instance. See [Appendix A](#app_a) for instructions on generating and downloading a .env file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Z1CS2Uwv79C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2027c92e-7703-4f31-9806-e3272ce05a05"
      },
      "source": [
        "# Load the numerapi credentials from .env or prompt for them if not available\n",
        "def credential():\n",
        "    dotenv_path = find_dotenv()\n",
        "    load_dotenv(dotenv_path)\n",
        "\n",
        "    if os.getenv(\"NUMERAI_PUBLIC_KEY\"):\n",
        "        print(\"Loaded Numerai Public Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_PUBLIC_KEY\"] = getpass(\"Please enter your Numerai Public Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_SECRET_KEY\"):\n",
        "        print(\"Loaded Numerai Secret Key into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_SECRET_KEY\"] = getpass(\"Please enter your Numerai Secret Key. You can find your key here: https://numer.ai/submit -> \")\n",
        "    \n",
        "    if os.getenv(\"NUMERAI_MODEL_ID\"):\n",
        "        print(\"Loaded Numerai Model ID into Global Environment!\")\n",
        "    else:\n",
        "        os.environ[\"NUMERAI_MODEL_ID\"] = getpass(\"Please enter your Numerai Model ID. You can find your key here: https://numer.ai/submit -> \")\n",
        "\n",
        "credential()\n",
        "public_key = os.environ.get(\"NUMERAI_PUBLIC_KEY\")\n",
        "secret_key = os.environ.get(\"NUMERAI_SECRET_KEY\")\n",
        "model_id = os.environ.get(\"NUMERAI_MODEL_ID\")\n",
        "napi = numerapi.NumerAPI(verbosity=\"info\", public_id=public_key, secret_key=secret_key)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded Numerai Public Key into Global Environment!\n",
            "Loaded Numerai Secret Key into Global Environment!\n",
            "Loaded Numerai Model ID into Global Environment!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w1LhPvUS7RLk"
      },
      "source": [
        "You can read up on the functionality of numerapi [here](https://github.com/uuazed/numerapi). You can use it to download the competition data, view other numerai users' public profiles, check submission status, manage your stake, and much more. In this case, we'll only be using it to download competition data and submit predictions.\n",
        "\n",
        "## 1.3 Downloading Competition Data\n",
        "In a more structured project, you'll probably want to keep the data in a seprate directory from your scripts etc. You could also link google colab to your google drive and store the data there in order to avoid needing to download and process the data every time. In this case, however, we'll keep everything in `./content`, and download the data fresh each time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VcCNcVMv7y1B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "outputId": "b2c8e14f-4b84-4de4-bccb-50d701fe4abd"
      },
      "source": [
        "napi.download_current_dataset()"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "./numerai_dataset_254.zip: 394MB [00:35, 21.9MB/s]                           \u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'./numerai_dataset_254.zip'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm7lo6fe8Qsy"
      },
      "source": [
        "## 1.4 Generating the Training Sample\n",
        "\n",
        "If you look at the files we downloaded above, you'll see a `numerai_tournament_data.csv` file and a `numerai_training_data.csv` file. The \"tournament\" file contains many rows with targets which we can use for validation, so let's extract those and combine them with our training set. Note that this cell saves a new `csv` after combining the training and validation data, so we can avoid the time-consuming parsing process if we run this cell again in the same session."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vd06RGi4840k",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "a7d80bff-6c2f-46ae-a285-736222242c82"
      },
      "source": [
        "tourn_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_tournament_data.csv')\n",
        "train_file = Path(f'./numerai_dataset_{napi.get_current_round()}/numerai_training_data.csv')\n",
        "processed_train_file = Path('./training_processed.csv')\n",
        "\n",
        "if processed_train_file.exists():\n",
        "    print(\"Loading the processed training data from file\\n\")\n",
        "    training_data = pd.read_csv(processed_train_file)\n",
        "else:\n",
        "    tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "    val_df = pd.concat([chunk[chunk['data_type'] == 'validation'] for chunk in tqdm(tourn_iter_csv)])\n",
        "    tourn_iter_csv.close()\n",
        "    training_data = pd.read_csv(train_file)\n",
        "    training_data = pd.concat([training_data, val_df])\n",
        "    training_data.reset_index(drop=True, inplace=True)\n",
        "    print(\"Training Dataset Generated! Saving to file ...\")\n",
        "    training_data.to_csv(processed_train_file, index=False)\n",
        "\n",
        "\n",
        "feature_cols = training_data.columns[training_data.columns.str.startswith('feature')]\n",
        "target_cols = ['target']\n",
        "\n",
        "train_idx = training_data.index[training_data.data_type=='train'].tolist()\n",
        "val_idx = training_data.index[training_data.data_type=='validation'].tolist()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading the processed training data from file\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-203d52ca0d80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mprocessed_train_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading the processed training data from file\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtraining_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_train_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtourn_iter_csv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtourn_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    686\u001b[0m     )\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 460\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    461\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1197\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1198\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1200\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2155\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m     \"\"\"\n\u001b[1;32m    532\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BSr1BMR5Ilh"
      },
      "source": [
        "# 2 Modeling the Data\n",
        "\n",
        "In this section, we will define our evaluation metrics; run two different models (a linear regression model from `scikit-learn` and a neural network from `fastai`); and generate submission dataframes from those files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeYfQcry6svF"
      },
      "source": [
        "## 2.1 Evaluation Metrics\n",
        "\n",
        "In this section, we will define two key evaluation metrics used to assess the performance of models before submitting to the tournament. These metrics are:\n",
        "- Average Spearman Correlation per era: The sum of each era's Spearman correlation divided by the number of eras.\n",
        "- Sharpe Ratio: The average correlation per era divided by the standard deviation of the correlations per era.\n",
        "\n",
        "Both are defined in reasonable detail [here](https://wandb.ai/carlolepelaars/numerai_tutorial/reports/How-to-get-Started-With-Numerai--VmlldzoxODU0NTQ). The methods defined below are modified versions of the methods described in that post."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtFniCX98z6i"
      },
      "source": [
        "def corr(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the correlation by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The average per-era correlations.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() \n",
        "\n",
        "def sharpe(df: pd.DataFrame) -> np.float32:\n",
        "    \"\"\"\n",
        "    Calculate the Sharpe ratio by using grouped per-era data\n",
        "    :param df: A Pandas DataFrame containing the columns \"era\", \"target\" and \"prediction\"\n",
        "    :return: The Sharpe ratio for your predictions.\n",
        "    \"\"\"\n",
        "    def _score(sub_df: pd.DataFrame) -> np.float32:\n",
        "        \"\"\" Calculate Spearman correlation for Pandas' apply method \"\"\"\n",
        "        return spearmanr(sub_df[\"target\"],  sub_df[\"prediction\"])[0]\n",
        "    corrs = df.groupby(\"era\").apply(_score)\n",
        "    return corrs.mean() / corrs.std()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4azmVvR6ZhT"
      },
      "source": [
        "## 2.2 Cross Validation Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnOaIHaW0rxi"
      },
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "train_sample = training_data.iloc[train_idx]\n",
        "X, y, era = train_sample[feature_cols], train_sample.target, train_sample.era\n",
        "groups = train_sample.era\n",
        "group_kfold = GroupKFold(n_splits = 5)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3qwFNYGF5TB"
      },
      "source": [
        "## 2.3 Linear Regression Model\n",
        "This model closely follows the tutorial example [here](https://colab.research.google.com/github/numerai/example-scripts/blob/master/making-your-first-submission-on-numerai.ipynb). We will use the `scikit-learn` package, with which we can implement and fit our regression model in just a couple of lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i-el1HtBefnb"
      },
      "source": [
        "#### Fitting the Linear Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKG75I7-yKvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca388973-13fb-4fd2-b1ed-ba359d838bdf"
      },
      "source": [
        "%%time\n",
        "corrs = []\n",
        "sharpes = []\n",
        "\n",
        "for train, test in group_kfold.split(X, y, groups=groups):\n",
        "    model = sklearn.linear_model.LinearRegression(n_jobs = -1)\n",
        "    model.fit(X.iloc[train], y.iloc[train])\n",
        "    val_preds = model.predict(X.iloc[test])\n",
        "    eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':y.iloc[test],\n",
        "                        'era':groups.iloc[test]}).reset_index()\n",
        "    corrs.append(corr(eval_df))\n",
        "    sharpes.append(sharpe(eval_df))\n",
        "\n",
        "print(corrs)\n",
        "print(sharpes)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.0413988755478284, 0.03116761404394065, 0.03960886988203633, 0.03559675523170117, 0.031243561514435816]\n",
            "[1.5032811853317405, 0.7154594471580004, 1.8264734992258658, 1.0864696284517816, 1.0273427745585324]\n",
            "CPU times: user 1min 18s, sys: 6.92 s, total: 1min 25s\n",
            "Wall time: 48.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fL4MLgxKYHB"
      },
      "source": [
        "models = [\n",
        "          # sklearn.linear_model.LinearRegression(n_jobs = -1),\n",
        "          # sklearn.linear_model.Lasso(alpha=0.00006), # good! Takes pretty long time.\n",
        "          # sklearn.linear_model.Lasso(alpha=0.00001), # Takes a long time, worse than .00005\n",
        "          # sklearn.linear_model.Lasso(alpha=0.000005), # Takes a really long time, worse than .00001\n",
        "          # sklearn.linear_model.Lasso(alpha=0.01), # fails\n",
        "          # sklearn.linear_model.Ridge(alpha=0.0001),\n",
        "          # sklearn.linear_model.Ridge(alpha=0.01),\n",
        "          # sklearn.linear_model.Ridge(alpha = 0.1),\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.5),  # Good!\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0002, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.4),\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.6), # Good! Best?\n",
        "          sklearn.linear_model.ElasticNet(alpha=.0001, l1_ratio=0.7) # Good! Best?\n",
        "\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.00005, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.0005, l1_ratio=0.5),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.00001, l1_ratio=0.25) # Takes very long time\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.01, l1_ratio=0.5), # fails\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.001, l1_ratio=0.25),\n",
        "          # sklearn.linear_model.ElasticNet(alpha=.001, l1_ratio=0.75) # bad result\n",
        "          ]"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RuK4TFtKX3l",
        "outputId": "2ede813b-8dde-415c-fe03-6c99ac07710e"
      },
      "source": [
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'\\nmodel: {model.__class__.__name__}')\n",
        "    if model.__class__.__name__!=\"LinearRegression\":\n",
        "        print(f'alpha: {model.alpha}')\n",
        "    if model.__class__.__name__==\"ElasticNet\":\n",
        "        print(f'L1 Ratio: {model.l1_ratio}')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
            "\n",
            " 20%|██        | 1/5 [01:09<04:36, 69.17s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [02:00<03:11, 63.85s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [03:08<02:10, 65.10s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [04:07<01:03, 63.18s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [05:17<00:00, 63.49s/it]\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: Lasso\n",
            "alpha: 6e-05\n",
            "validation correlations: [0.04332792912546912, 0.031434612619569154, 0.047669425200041284, 0.037039767734678784, 0.033323948348046604], mean: 0.03855913660556099\n",
            "validation sharpes: [1.3982035171750715, 0.663410919086826, 2.0610600121461347, 0.9854986745433042, 0.9674671028213477], mean: 1.215128045154537\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:26<01:47, 26.77s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [00:50<01:17, 25.97s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [02:00<01:18, 39.11s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [02:24<00:34, 34.66s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [03:28<00:00, 41.75s/it]\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.5\n",
            "validation correlations: [0.04347474676845641, 0.03146511092526201, 0.047107530862073116, 0.03730122819900296, 0.03341224276698343], mean: 0.03855217190435559\n",
            "validation sharpes: [1.416323394305138, 0.6677888412291337, 2.0469916387510954, 1.0044445499519083, 0.9793182577424634], mean: 1.2229733363959476\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:42<02:49, 42.30s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [01:12<01:55, 38.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [01:56<01:20, 40.41s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [03:07<00:49, 49.43s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [04:27<00:00, 53.56s/it]\n",
            "\n",
            "\n",
            "  0%|          | 0/5 [00:00<?, ?it/s]\u001b[A\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.6\n",
            "validation correlations: [0.04333048511430412, 0.0314392864379456, 0.04768023730348006, 0.037036624347257956, 0.033322480554536356], mean: 0.03856182275150481\n",
            "validation sharpes: [1.398431288545246, 0.6634818817752716, 2.061180211216678, 0.985120477558037, 0.9673392231760424], mean: 1.2151106164542549\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            " 20%|██        | 1/5 [00:43<02:54, 43.65s/it]\u001b[A\u001b[A\n",
            "\n",
            " 40%|████      | 2/5 [01:38<02:20, 46.93s/it]\u001b[A\u001b[A\n",
            "\n",
            " 60%|██████    | 3/5 [02:50<01:49, 54.66s/it]\u001b[A\u001b[A\n",
            "\n",
            " 80%|████████  | 4/5 [03:21<00:47, 47.56s/it]\u001b[A\u001b[A\n",
            "\n",
            "100%|██████████| 5/5 [04:35<00:00, 55.07s/it]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model: ElasticNet\n",
            "alpha: 0.0001\n",
            "L1 Ratio: 0.7\n",
            "validation correlations: [0.04306784559555605, 0.031391661906845475, 0.047987359704428266, 0.03667405756155036, 0.033069786367601856], mean: 0.038438142227196395\n",
            "validation sharpes: [1.375155679465384, 0.6616136201502878, 2.0557192796959773, 0.965906404669539, 0.9535450170076238], mean: 1.2023880001977623\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLxlDiV_Q6ao"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "# Takes a very long time\n",
        "import sklearn.neighbors\n",
        "models = [        \n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, n_jobs = -1),\n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, leaf_size = 150, n_jobs = -1),\n",
        "          sklearn.neighbors.KNeighborsRegressor(n_neighbors = 500, leaf_size = 10, n_jobs = -1)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__}')\n",
        "    print(f'neighbors: {model.n_neighbors}')\n",
        "    print(f'leaf size: {model.leaf_size}')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbvd-j2Xj3EZ"
      },
      "source": [
        "%%script false --no-raise-error\n",
        "# Takes a very long time\n",
        "import sklearn.ensemble\n",
        "models = [\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=5, max_features = 0.8, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=10, max_features = 0.8, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=5, max_features = 0.2, n_jobs=-1),\n",
        "          sklearn.ensemble.ExtraTreesRegressor(n_estimators=10, max_features = 0.2, n_jobs=-1),\n",
        "          # sklearn.ensemble.ExtraTreesRegressor(n_estimators=100, max_features = 0.8, n_jobs=-1),\n",
        "          # sklearn.ensemble.ExtraTreesRegressor(n_estimators=1000, max_features = 0.8, n_jobs=-1)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__} with {model.n_estimators} estimators and {model.max_features} max features.')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Af4VMqC9r7tY",
        "outputId": "8a02caec-a558-4b28-adb1-bd6fe21ddf8f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "import sklearn.ensemble\n",
        "models = [\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=50, max_features = 0.8, subsample=0.1),\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, max_features = 0.8),\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=50, max_features = 0.2),\n",
        "          sklearn.ensemble.GradientBoostingRegressor(n_estimators=100, max_features = 0.2)\n",
        "                    ]\n",
        "\n",
        "for model in models:\n",
        "    corrs = []\n",
        "    sharpes = []\n",
        "    for train, test in tqdm(group_kfold.split(X, y, groups=groups), total=5):\n",
        "        model.fit(X.iloc[train], y.iloc[train])\n",
        "        val_preds = model.predict(X.iloc[test])\n",
        "        eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                            'target':y.iloc[test],\n",
        "                            'era':groups.iloc[test]}).reset_index()\n",
        "        corrs.append(corr(eval_df))\n",
        "        sharpes.append(sharpe(eval_df))\n",
        "    print(f'model: {model.__class__.__name__} with {model.n_estimators} estimators and {model.max_features} max features.')\n",
        "    print(f'validation correlations: {corrs}, mean: {np.array(corrs).mean()}')\n",
        "    print(f'validation sharpes: {sharpes}, mean: {np.array(sharpes).mean()}\\n')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-de2b0775e7fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0msharpes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup_kfold\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mval_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         eval_df = pd.DataFrame({'prediction':val_preds,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1535\u001b[0m         n_stages = self._fit_stages(\n\u001b[1;32m   1536\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rng\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1537\u001b[0;31m             sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1538\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1539\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1592\u001b[0m             raw_predictions = self._fit_stage(\n\u001b[1;32m   1593\u001b[0m                 \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1594\u001b[0;31m                 random_state, X_idx_sorted, X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1595\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1596\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/ensemble/_gb.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1243\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1245\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1247\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1224\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1225\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1226\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    365\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inUbdhvb66Pb"
      },
      "source": [
        "#### Assessing Regression Model Performance\n",
        "\n",
        "Here we apply the `corr` and `sharpe` methods defined above to predictions made on the validation sample in order to estimate our model's tournament performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgU0Pz9x9vpv"
      },
      "source": [
        "val_sample = training_data.iloc[val_idx]\n",
        "val_preds = model.predict(val_sample[feature_cols])\n",
        "eval_df = pd.DataFrame({'prediction':val_preds,\n",
        "                        'target':val_sample.target,\n",
        "                        'era':val_sample.era}).reset_index()\n",
        "val_sharpe = sharpe(eval_df)\n",
        "val_corr = corr(eval_df)\n",
        "\n",
        "print((f'The linear regression model\\'s validation correlation is {val_corr}. '\n",
        "       f'Its validation sharpe is {val_sharpe}'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRLGopZF0r7T"
      },
      "source": [
        "#### Making Predictions with the Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nOeJjJdZ08aV"
      },
      "source": [
        "ids = []\n",
        "preds = []\n",
        "\n",
        "chunksize = 50000\n",
        "\n",
        "tourn_iter_csv = pd.read_csv(tourn_file, iterator=True, chunksize=1e6)\n",
        "for chunk in tourn_iter_csv:\n",
        "    df = chunk[feature_cols]\n",
        "    out = model.predict(df)\n",
        "    ids.extend(chunk[\"id\"])\n",
        "    preds.extend(out)\n",
        "tourn_iter_csv.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9HN8lrw2zd_"
      },
      "source": [
        "linear_regression_predictions_df = pd.DataFrame({\n",
        "    'id':ids,\n",
        "    'prediction':preds\n",
        "})\n",
        "linear_regression_predictions_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg_HdPki6Mh3"
      },
      "source": [
        "linear_regression_predictions_df.to_csv(\"lr_predictions.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEwbRegtWrRE"
      },
      "source": [
        "#### Submitting Predictions from the Linear Regression Model\n",
        "\n",
        "We can use `numerapi` to submit these predictions as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW-zccdgW0GX"
      },
      "source": [
        "napi.upload_predictions(\"lr_predictions.csv\", model_id=os.environ.get(\"NUMERAI_MODEL_ID\"))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}